// ==============================
// ğŸ”¥ ë†í˜‘ ì˜ìˆ˜ì¦ ìµœì í™” OCR ì„œë²„
// ==============================
import cors from "cors";
import dotenv from "dotenv";
import express from "express";
import fs from "fs";
import multer from "multer";
import fetch from "node-fetch";
import OpenAI, { APIUserAbortError } from "openai";

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json());

const imageupload = multer({ dest: "uploads/" });
const port = 5000;
const API_KEY = "AIzaSyAnwvS3jcDO610aSMIz2wzfycJAGKFVBA4";
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const GOOGLE_API_KEY = API_KEY;


// ==============================
// ğŸ“Œ Vision v1p4beta1 OCR (ë†í˜‘ ì˜ìˆ˜ì¦ ìµœì í™”)
// ==============================
async function requestVisionOCR(base64Image) {
  const response = await fetch(
    `https://vision.googleapis.com/v1p4beta1/images:annotate?key=${GOOGLE_API_KEY}`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        requests: [
          {
            image: { content: base64Image },
            features: [{ type: "DOCUMENT_TEXT_DETECTION" }],
            imageContext: {
              languageHints: ["ko"],
              textDetectionParams: {
                enableTextDetectionConfidenceScore: true
              }
            }
          }
        ],
      }),
    }
  );

  const data = await response.json();

  // confidence ìˆëŠ” ë¼ì¸ë§Œ ëª¨ìœ¼ê¸°
  const textBlocks =
    data.responses?.[0]?.textAnnotations?.[0]?.description || "";

  return textBlocks;
}


// ==============================
// ğŸ“Œ OCR ì „ì²˜ë¦¬
// ==============================
function preprocessOCR(text) {
  let cleaned = text;

  // ë¬¸ì+ìˆ«ì ë¶„ë¦¬
  cleaned = cleaned.replace(/([ê°€-í£]+)(\d)/g, "$1 $2");

  // ìˆ«ì+ë¬¸ì ë¶„ë¦¬
  cleaned = cleaned.replace(/(\d)([ê°€-í£]+)/g, "$1 $2");

  // 3.300 â†’ 3300
  cleaned = cleaned.replace(/(\d+)[.,](\d{3})/g, "$1$2");

  // ê³µë°± ì •ë¦¬
  cleaned = cleaned.replace(/\s+/g, " ");

  return cleaned.trim();
}


// ==============================
// ğŸ“Œ GPT 1ì°¨ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
// ==============================
function buildExtractPrompt(text) {
  return `
ë‹¤ìŒ OCR í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ìƒí’ˆëª…(ItemName)ê³¼ ìˆ˜ëŸ‰(ItemCount)ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

â— ë§¤ìš° ì¤‘ìš”
- OCR í…ìŠ¤íŠ¸ì— **ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒí’ˆì„ ìƒì„±í•˜ë©´ ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.**
- OCRì—ì„œ ë³´ì´ì§€ ì•Šì€ ë‹¨ì–´(ë‹¹ê·¼, ë°°ì¶” ë“±)ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ê°€ê²©ì€ ì œì™¸í•˜ì„¸ìš”.
- ìƒí’ˆëª… ì•ì— pëŠ” ì œê±°í•˜ì„¸ìš”
- ìƒí’ˆëª… ì˜†ì— ë‹¨ìœ„ë¥¼ ë¹¼ì§€ë§ˆì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, ml,g,kgê°™ì€ ë‹¨ìœ„ë¥¼ ë¹¼ì§€ë§ˆì„¸ìš”
- ìƒí’ˆëª…ì´ í•œ ê¸€ìì—¬ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ("ë¬´", "íŒŒ")
- ìˆ˜ëŸ‰ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1

ì¶œë ¥ í˜•ì‹:
[
  { "ItemName": "", "ItemCount": "" }
]

OCR:
${text}
`;
}


// ==============================
// ğŸ“Œ GPT í˜¸ì¶œ
// ==============================
async function askGPT(prompt) {
  const res = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
  });

  let text = res.choices[0].message.content.replace(/```json|```/g, "").trim();
  const start = text.indexOf("[");
  const end = text.lastIndexOf("]");

  if (start === -1 || end === -1) return [];
  try {
    return JSON.parse(text.substring(start, end + 1));
  } catch {
    return [];
  }
}


// ==============================
// ğŸ“Œ ìµœì¢… /ocr ì—”ë“œí¬ì¸íŠ¸
// ==============================
app.post("/ocr", imageupload.single("image"), async (req, res) => {
  try {
    const imagePath = req.file.path;
    const base64Image = fs.readFileSync(imagePath).toString("base64");

    // ğŸ”¥ ë†í˜‘ ì˜ìˆ˜ì¦ìš© OCR v1p4beta1
    const ocrText = await requestVisionOCR(base64Image);

    console.log("==== ì›ë³¸ OCR ====");
    console.log(ocrText);

    // OCR ì‹¤íŒ¨ ì‹œ ë°”ë¡œ ì¢…ë£Œ
    if (!ocrText || ocrText.trim().length < 5) {
      return res.json({
        error: "OCRì´ ì˜ìˆ˜ì¦ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì°ì–´ì£¼ì„¸ìš”.",
        parsed: [],
      });
    }

    const cleaned = preprocessOCR(ocrText);

    console.log("==== ì „ì²˜ë¦¬ í›„ OCR ====");
    console.log(cleaned);

    // GPTë¡œ ì‹¤ì œ í•­ëª© ì¶”ì¶œ
    const prompt = buildExtractPrompt(cleaned);
    const final = await askGPT(prompt);

    console.log("==== ìµœì¢… GPT ê²°ê³¼ ====");
    console.log(final);

    res.json({
      rawOCR: ocrText,
      cleanedOCR: cleaned,
      parsed: final,
    });

    fs.unlinkSync(imagePath);

  } catch (err) {
    console.error(err);
    res.status(500).json({ error: "OCR ì²˜ë¦¬ ì‹¤íŒ¨" });
  }
});


app.listen(port, () => {
  console.log(`ì„œë²„ ì‹¤í–‰: http://localhost:${port}`);
});
